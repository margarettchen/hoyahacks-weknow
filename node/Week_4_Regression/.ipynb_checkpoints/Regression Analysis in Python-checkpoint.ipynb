{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis in Python\n",
    "\n",
    "*This lesson is adapted from [Penn State's Regression Lesson Plan](https://onlinecourses.science.psu.edu/stat501/node/250)*\n",
    "\n",
    "**Regression** is a statistical technique that tries to measure the strength of a relationship between a single **dependent variable (response)** and one or more **independent variables (predictors)**. Regression is an example of using supervised learning for continuous data. \n",
    "\n",
    "Regression is a popular technique in data science for prediction and forecasting because:\n",
    "1. Speedy analysis (relative to other ML methods)\n",
    "2. Highly explainable\n",
    "3. Easy to run \n",
    "4. Generally robust prediction technique, especially with multiple predictors\n",
    "\n",
    "## Simple Linear Regression\n",
    "In simple linear regression, there are two variables: an \"x\", which is the single predictor variable, and a \"y\" , which is the single response variable. With simple linear regression, we are looking at a nondeterministic relationship between the x and the y. \n",
    "\n",
    "In general, we are looking for some kind of \"trend\" between the two variables. \n",
    "\n",
    "Let's import a couple of packages to view some graphs. \n",
    "Don't worry about the code for now! We'll go over that soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,50)\n",
    "y = 10 + 3*x + np.random.normal(0, 10, 50)\n",
    "plt.xlabel(\"x\", fontsize=25)\n",
    "plt.ylabel(\"y\", fontsize=25)\n",
    "plt.title(\"Positive trend\",fontsize=25)\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,50)\n",
    "y = 10 - 3*x + np.random.normal(0, 10, 50)\n",
    "plt.xlabel(\"x\", fontsize=25)\n",
    "plt.ylabel(\"y\", fontsize=25)\n",
    "plt.title(\"Negative trend\",fontsize=25)\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,50)\n",
    "y = np.random.randint(0,50, 50)\n",
    "plt.xlabel(\"x\", fontsize=25)\n",
    "plt.ylabel(\"y\", fontsize=25)\n",
    "plt.title(\"No trend\",fontsize=25)\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For statistical relationships with a trend, we can graph a line that best fits the points. This line is literally called the **line of best fit**, a very fitting name indeed. This line may not necessarily hit all the points in the graph, but it will be pretty close, depending on how strong the trend is. \n",
    "\n",
    "To understand the formula for the line of best fit, we must learn some terminology.\n",
    "\n",
    "* x<sub>i</sub> is an individual predictor value \n",
    "* y<sub>i</sub> is an individual observed response value (the \"actual\" value)\n",
    "* ŷ<sub>i</sub> is an indididual predicted response values (\"the \"fitted\" value)\n",
    "\n",
    "(x<sub>i</sub>, y<sub>i</sub>) = actual data point for a given x<sub>i</sub>\n",
    "\n",
    "(x<sub>i</sub>, ŷ<sub>i</sub>) = predicted data point for a given x<sub>i</sub>\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "The formula for the line of best fit is ŷ<sub>i</sub> = b<sub>0</sub> + b<sub>1</sub> * x<sub>i</sub>\n",
    "\n",
    "* b<sub>0</sub> is the ŷ-intercept (value when x is 0)\n",
    "* b<sub>1</sub> is the slope of the line of best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0,15)\n",
    "y = np.array([2,7,6,9,13,11,14,17,14,19,22,23,26,27,29])\n",
    "plt.xlabel(\"x\", fontsize=25)\n",
    "plt.ylabel(\"y\", fontsize=25)\n",
    "plt.scatter(x, y)\n",
    "\n",
    "params = np.polyfit(x,y,1)\n",
    "slope = params[0]\n",
    "y_intercept = params[1]\n",
    "\n",
    "# best fit line\n",
    "y_hat = y_intercept+slope*x\n",
    "\n",
    "plt.plot(x, y_hat, color=\"red\")\n",
    "\n",
    "print()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares \n",
    "So how does np.polyfit work? \n",
    "\n",
    "There are several ways to fit a set of data points and create a line of best fit. We'll focus on the method of **ordinary least squares (OLS) ** to figure out the slope and y-intercept of the best fit line.\n",
    "\n",
    "To understand OLS, we have to first understand the **residual error**, which is the difference between the observed response and predicted response. \n",
    "\n",
    "e<sub>i</sub> = y<sub>i</sub>- ŷ<sub>i</sub> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([8],[14])\n",
    "plt.plot([8,8], [14,y_hat[8]], color=\"green\")\n",
    "plt.plot(x, y_hat, color=\"red\")\n",
    "print(\"Residual is: \" + str(14-y_hat[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x,y)\n",
    "residuals = np.array([])\n",
    "for i in range(15):\n",
    "    plt.plot([i,i], [y[i],y_hat[i]], color=\"green\")\n",
    "    np.append(residuals, y[i]-y_hat[i])\n",
    "plt.plot(x, y_hat, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we really want to do for OLS is minimize the sum of the squares of the residual residuals (SSE). This is bascially minimizing the errors, so the line is really best fit.\n",
    "\n",
    "SSE = Σ [(y<sub>i</sub> - ŷ<sub>i</sub>)<sup>2</sup>] = Σ (e<sub>i</sub>)<sup>2</sup>\n",
    "\n",
    "We use squared errors because \n",
    " 1. Everything is positive (squares are always positive)\n",
    " 2. It imposes a \"penalty\" for larger errors\n",
    "     * A residual that is of size 2 has a penalty of 4\n",
    "     * A residual that is of size 3 has a penalty of 9\n",
    "     * Though 3 is one unit away from 2, the penalty for 3 is much stronger than that of 2\n",
    " \n",
    " \n",
    "With OLS, we can derive formulas to find the slope and y-intercept for the best-fitting line. \n",
    "\n",
    "b<sub>1</sub> = Σ [(x<sub>i</sub>-x̅)*(y<sub>i</sub>-ȳ)] /  Σ (x<sub>i</sub>-x̅)<sup>2</sup>\n",
    "\n",
    "b<sub>0</sub> = ȳ - b<sub>1</sub> * x̅\n",
    "\n",
    "\n",
    "Here's the derivation for the formulas if you are interested. You will need to have a good understanding of calculus: [proof](https://isites.harvard.edu/fs/docs/icb.topic515975.files/OLSDerivation.pdf) \n",
    "    \n",
    "___\n",
    "\n",
    "### Exercise\n",
    "Let's get back to coding! \n",
    "In the Week 4 folder, check out the Excel file called \"crickets\", which I got from this [site](http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/frame.html). Figure out what the appropriate x and y variables are, graph the relationship (label your axes please!), and state the formula for line of best fit in the title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Determination (r<sup>2 </sup>)\n",
    "In many cases, we have to know how strongly (or weakly) the best fit line fits the data. We can quantify how strong the regression line fits the data using the coefficient of determination, also known as r<sup>2</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to pip install scipy\n",
    "from scipy import stats\n",
    "\n",
    "x = np.arange(0,50)\n",
    "y = 10 + 3*x + np.random.normal(0, 10, 50)\n",
    "\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html\n",
    "r_squared = stats.linregress(x,y)[2]\n",
    "\n",
    "plt.xlabel(\"x\", fontsize=25)\n",
    "plt.ylabel(\"y\", fontsize=25)\n",
    "plt.title(\"R squared is \"+'{:.3f}'.format(r_squared),fontsize=25)\n",
    "plt.scatter(x, y)\n",
    "\n",
    "params = np.polyfit(x,y,1)\n",
    "slope = params[0]\n",
    "y_intercept = params[1]\n",
    "\n",
    "y_hat = y_intercept+slope*x\n",
    "\n",
    "# best fit line\n",
    "plt.plot(x, y_hat, color=\"red\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://image.slidesharecdn.com/linearregression-140903114216-phpapp01/95/linear-regression-22-638.jpg?cb=1409744639)\n",
    "[Source](https://image.slidesharecdn.com/linearregression-140903114216-phpapp01/95/linear-regression-22-638.jpg?cb=1409744639) \n",
    "\n",
    "Coefficient of Determination (r<sup>2</sup>) : \n",
    "** r<sup>2</sup> = 1 - (SSE / SSTO) **\n",
    "* The range of r<sup>2</sup> is between 0 and 1\n",
    "* If r<sup>2</sup> is 1, then variation in y is *totally* accounted by the x-variable\n",
    "    * All data lies perfectly on the best-fit line\n",
    "* If r<sup>2</sup> is 0, then variation in y is *not at all* accounted by the x-variable\n",
    "    * The regression line is horizontal \n",
    "* **Interpretation** \n",
    "    * \"r<sup>2</sup> ×100 percent of the variation in y is 'explained by' the variation in predictor x.\"\n",
    "        * Remember ***correlation does not imply causation***\n",
    "    * \"r<sup>2</sup> ×100 percent of the variation in y is reduced by taking into account predictor x\"\n",
    "\n",
    "Correlation Coefficient (r):\n",
    "* Measure of the linear correlation between two variables, x and y\n",
    "* r = +/- Sqrt(r<sup>2</sup>)\n",
    "    * Sign depends on the slope of regression line\n",
    "        * If the slope is positive, then r is positive.\n",
    "        * If the slope is negative, then r is negative.\n",
    "    * Range of r is between -1 and 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Assumptions for Linear Regression\n",
    "1. Data must be linearly related. \n",
    "    * The graph should look linear. No curves.\n",
    "    * Mean of e<sub>i</sub> is 0\n",
    "2. The errors are independent\n",
    "3. Errors are normally distributed\n",
    "4. Errors have constant variance \n",
    "\n",
    "\n",
    "## Application of Regression in ML\n",
    "Code source adapted from [SKLearn Example](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html) \n",
    "Data source: [Car MPG Analysis](https://archive.ics.uci.edu/ml/datasets/auto+mpg) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "auto_data = pd.read_csv(\"auto_data.csv\")\n",
    "print(auto_data.head())\n",
    "print()\n",
    "print(\"The shape of the data is: \" + str(auto_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Can we predict mpg from weight? \n",
    "Let weight be the predictor (x) and mpg be the response (y). \n",
    "\n",
    "We're also going to pretend that we don't have all the data. We want to predict 20% of our data using 80% as a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Randomize and split our data\n",
    "train, test = train_test_split(auto_data, test_size = 0.2)\n",
    "\n",
    "train_x = train[['weight']]\n",
    "train_y = train[['mpg']]\n",
    "\n",
    "test_x = test[['weight']]\n",
    "test_y = test[['mpg']]\n",
    "\n",
    "# Create linear regression object\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Train the model ('fit' synonomous to 'train')\n",
    "regr.fit(train_x, train_y)\n",
    "\n",
    "plt.scatter(train_x, train_y,  color='black')\n",
    "plt.scatter(test_x, test_y,  color='red')\n",
    "\n",
    "# predicted best-fit line in blue\n",
    "plt.plot(test_x, regr.predict(test_x), color='blue',\n",
    "         linewidth=3)\n",
    "\n",
    "plt.xlabel(\"weight\", fontsize=15)\n",
    "plt.ylabel(\"mpg\", fontsize=15)\n",
    "\n",
    "print('R squared value is: %.2f' % r2_score(test_y, regr.predict(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you interpret the r<sup>2</sup> value? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
